
1. **Aho-Corasick Algorithm**: O(n + m + z) where n is the length of the text, m is the total length of all patterns, and z is the number of pattern occurrences found.

2. **Bellman-Ford Algorithm**: O(VE) where V is the number of vertices and E is the number of edges.

3. **Borůvka's Algorithm**: O(E log V) for constructing a minimum spanning tree.

4. **Dinic's Algorithm**: O(V^2E) for general networks and O(E√V) for unit capacity networks.

5. **Edmonds-Karp Algorithm**: O(VE^2) for finding maximum flow.

6. **Floyd-Warshall Algorithm**: O(V^3) for all pairs shortest paths.

7. **Ford-Fulkerson Algorithm**: O(max_flow * E) where max_flow is the maximum flow in the network.

8. **Gabow's Algorithm for Strongly Connected Components**: O(V + E).

9. **Hopcroft-Karp Algorithm**: O(√V E) for finding maximum matching in a bipartite graph.

10. **Knuth-Morris-Pratt Algorithm (KMP)**: O(n + m) where n is the length of the text and m is the length of the pattern.

11. **Kosaraju's Algorithm**: O(V + E).

12. **Manacher's Algorithm**: O(n) for finding the longest palindromic substring.

13. **Maximum Flow Algorithms**: Various complexities depending on the implementation (e.g., Edmonds-Karp: O(VE^2)).

14. **Minimum Cost Maximum Flow Algorithm**: O(VE log V) using the cycle cancelling method or successive shortest path algorithm.

15. **Rabin-Karp Algorithm**: Average O(n + m), worst case O(nm) due to hash collisions.

16. **Stoer-Wagner Algorithm**: O(V^3) for finding minimum cut.

17. **Suffix Array Construction**: O(n log n) for suffix sorting.

18. **Tarjan's Algorithm for Strongly Connected Components**: O(V + E).

19. **Tarjan's Offline Lowest Common Ancestor Algorithm**: O(n + qα(n)) where α is the inverse Ackermann function, n is the number of nodes, and q is the number of queries.

20. **Z Algorithm**: O(n + m).

21. **Dynamic Programming with Bitmasks**: O(n * 2^n) for traveling salesman problem solutions.

22. **Segment Tree with Lazy Propagation**: O(log n) for updates and queries.

23. **Fenwick Tree (Binary Indexed Tree)**: O(log n) for updates and queries.

24. **Square Root Decomposition**: O(sqrt(n)) for range queries and updates.

25. **Persistent Segment Tree**: O(log n) per update and query, providing versions of the data structure.

26. **Link-Cut Tree**: O(log n) for dynamic tree operations.

27. **Heavy-Light Decomposition**: O(log^2 n) for path queries and updates.

28. **Convex Hull Trick**: O(log n) per line query and insertion.

29. **Fast Fourier Transform (FFT)**: O(n log n) for computing the DFT.

30. **Number Theoretic Transform (NTT)**: O(n log n) similar to FFT but over modular arithmetic.

31. **Kademlia Algorithm**: O(log n) for peer-to-peer network operations.

32. **Dominator Tree**: O(V log V + E) or faster using data structure optimizations.

33. **Voronoi Diagram**: O(n log n).

34. **Delaunay Triangulation**: O(n log n).

35. **Graham Scan**: O(n log n) for constructing convex hulls.

36. **Gale-Shapley Algorithm (Stable Marriage Problem)**: O(n^2).

37. **K-d Tree**: O(log n) average case for insertions, deletions, and searches.

38. **Interval Tree**: O(log n) for insertion and querying.

39. **Network Simplex Algorithm**: Polynomial time, typically faster in practice for minimum cost flow.

40. **Quadtree**: O(log n) average case for insertion and search.

41. **Treap**: O(log n) for insertion, deletion, and search.

42. **Splay Tree**: O(log n) amortized for access operations.

43. **Meet in the Middle**: O(2^(n/2)) for subset sum and other combinatorial problems.

44. **Simulated Annealing**: Varies; typically polynomial for approximations.

45. **Euler Tour Technique**: O(n) for preprocessing and O(1) queries.

46. **Christofides Algorithm**: O(n^2) for approximating the solution to TSP.

47. **Kadane's Algorithm**: O(n) for maximum subarray sum.

48. **Ternary Search Algorithm**: O(log n) for unimodal function optimization.

49. **Bloom Filter**: O(1) for insertion and queries with false positives.

50. **Byzantine Generals Problem**: Varies based on implementations and assumptions about the environment.

51. **Critical Path Method**: O(V + E) for determining project duration.

52. **Boyer-Moore Algorithm**: O(n/m) average-case for pattern matching.

53. **Needleman-Wunsch Algorithm**: O(nm) for sequence alignment.

54. **Pollard's rho Algorithm**: O(√N) for factorization.

55. **Miller-Rabin Primality Test**: O(k log^3 n) for k iterations of the test.

56. **Monge's Theorem Algorithm**: Contextual, related to matrix optimization.

57. **Schwarz–Christoffel Mapping Algorithm**: Complex; calculated case-by-case for conformal mappings.

58. **Tower of Hanoi Algorithm**: O(2^n) recursive solution.

59. **Huffman Encoding Algorithm**: O(n log n) for building the Huffman tree.

60. **Heapsort**: O(n log n) for sorting.

61. **Minimum Spanning Tree**: O(E log V) using Kruskal's or Prim's algorithm.

62. **Edit Distance**: O(nm) using dynamic programming.

63. **Linear Regression**: O(nm^2) for ordinary least squares with m parameters and n examples.

64. **Gradient Descent**: O(nt) where n is data size, t is number of iterations.

65. **Random Forest**: O(mn log n) where m is the number of trees and n is number of samples.

66. **K-means**: O(nkt) for n data points, k clusters, t iterations.

67. **Neural Network**: Varies; depends on architecture and dataset size.

68. **Support Vector Machine**: O(n^2) to O(n^3) for training, depending on implementation.

69. **Apriori Algorithm**: O(2^n) exponential, to find frequent itemsets.

70. **Prim's Algorithm**: O(E log V) to find MST.

71. **Dijkstra's Algorithm**: O(E + V log V) using priority queues.

72. **Quick Sort**: O(n^2) worst-case, O(n log n) average-case.

73. **Breadth First Search (BFS)**: O(V + E).

74. **Depth First Search (DFS)**: O(V + E).

75. **A* search algorithm**: O(E + V log V) in a graph with admissible heuristic.

76. **Kruskal's Algorithm**: O(E log E) to find MST.

77. **Greedy Algorithm**: Varies per specific implementation.

78. **Genetic Algorithm**: Varies; generally O(generation size * number of generations).

79. **Merge Sort**: O(n log n).

80. **Bubble Sort**: O(n^2).

81. **Selection Sort**: O(n^2).

82. **Binary Search**: O(log n).

83. **Topological Sort**: O(V + E).

84. **LRU Cache Replacement**: O(1) with linked hash map approach.

85. **Traveling Salesman Problem Algorithms**: Exponential; varies with approach (e.g. O(n^2 * 2^n) for dynamic programming).

86. **Branch and Bound**: Varies; typically exponential for exact solutions.

87. **PageRank Algorithm**: O((V + E) log(1/ε)) for convergence to ε tolerance.

88. **Longest Common Subsequence Algorithm**: O(nm) using dynamic programming.

89. **RSA Algorithm**: O((log n)^3) for encryption/decryption operations.

90. **Radix Sort**: O(nk) where k is the length of the longest number.

91. **Tabu Search**: Varies; heuristic optimization method.

92. **Monte Carlo Algorithm**: Varies; depends on accuracy and convergence.

93. **Coin Change Problem**: O(n * amount) using dynamic programming.

94. **Dynamic Programming**: Varies per problem; generally polynomial time complexity.

95. **Divide and Conquer**: Varies per problem; log n factor for partitioning.

96. **Backtracking**: Varies; typically exponential for combinatorial problems.

97. **Hill Climbing**: Varies; can be non-polynomial.

98. **Expectation-Maximization Algorithm**: Varies; typically iterative until convergence.

99. **Ant Colony Optimization**: Varies; heuristic algorithm for optimization.

100. **Fisher-Yates Shuffle Algorithm**: O(n) for shuffling.

101. **Johnson's Algorithm**: O(V^2 log V + VE).

102. **Trie Data Structure**: O(m) for search/insert where m is the length of the word.

103. **Max-flow Min-cut Theorem**: Relates to the cost of solving the flow problem (like Edmonds-Karp's complexity).

104. **Berlekamp-Massey Algorithm**: O(n^2) for a sequence of length n.

105. **Hungarian Algorithm**: O(n^3) for solving assignment problem.

106. **Clique Enumeration Algorithm**: Typically NP-complete.

107. **Mutual Exclusion Algorithm**: Varies; based on implementation.

108. **Centroid Decomposition**: O(n log n) for tree operations.

109. **Sparse Table**: O(n log n) preprocessing, O(1) queries.

110. **Euler's Totient Function**: O(sqrt(n)) for computation.

111. **AVL Tree**: O(log n) for insertion, deletion, and lookup.

112. **Skip List**: O(log n) average case for insertion/search.

113. **Mo's Algorithm**: O((n + q)√n) for q queries on an array of size n.

114. **Disjoint Set Union**: O(α(n)) per operation, where α is the inverse Ackermann function.

115. **String Algorithms**: Specific to the algorithm, e.g., KMP: O(n+m).

116. **Computational Geometry Algorithms**: Specific per task, e.g., Convex Hull: O(n log n).

117. **Approximation Algorithms**: Algorithm-specific, often polynomial.

118. **Quantum Algorithms**: Varies; Shor's: polynomial for factoring.